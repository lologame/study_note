1.hadoop的自定义类设计
	以自定义数据进行mapredce的节点间传输，该自定义数据类型，需要遵循hadoop的序列化机制。
	也就是就必须实现hadoop序列化接口:Writable。
	这是因为hadoop集群并没有这些自定义数据结构，因此为了让
	package FlowSum.mapreduce;
	代码：
	public class FlowBean implements WritableComparable<FlowBean>{

		public FlowBean(){}		//为了能够反射，因此必须要有默认ctor
		
		
		//序列化输出时，分别将这些进行序列化。
		@Override
		public void write(DataOutput out) throws IOException {
			//sequence
			out.writeUTF(string);
			out.writeLong(longdata);
		}
		
		//序列化输入时，分别将这些反序列化，注意顺序一定要和wirte时，一致
		@Override
		public void readFields(DataInput in) throws IOException {
			string = in.readUTF();
			longdata = in.readLong();
		}
		
			@Override
		public int compareTo(FlowBean o) {
			return sumflow>o.getSumFlow() ? -1 : 1;
		}
	}
	因为很可能用FlowBean作为关键词key，关键词必须实现WritableComparable接口，这样才具备排序功能。
	不能使用Writable 和 Comparable进行组合，必须要写在一起，否则无法正常工作。
	
2.reduce分组
	需要自定义改造两个机制:
		1).改造分区逻辑，自定义一个partioner
		2).自定义reducer task的并发任务数。
	代码：
		1).编写分组逻辑（分组逻辑，一个组对应一个reducer）
		public class AreaPartitioner<KEY, VALUE> extends Partitioner<KEY, VALUE>{
			
			static {
				//将分组依据进行初始化，比如从数据库或文件的数据对应关系 导入到内存。
			}
			
			private int getNumber(KEY k, VALUE v){
				//code
			}
			
			@Override
			public int getPartition(KEY key, VALUE value, int numPartitions) {
				//
				return getNumber(key, value);
			}
		}
	
		2).配置分组类
		job.setPartitionerClass(AreaPartitioner.class);
		job.setNumReduceTasks(ReducerNumber);				//这个ReducerNumber一定要和分组数一样。少于分组数，会报错。大于分组数会浪费。
		注意,当ReducerNumber==1时，强制性分组为1（因为reducerNumber==1时，hadoop知道分组无意义）

3.shuffle机制
	1).shuffle的意思就是洗牌。
	                                                                        MAPPER    |      REDUCER
																			          |
                                            |--> file1               partition2       |
	input split -> MAP -> buffer in memory ----> file2------> fetch------------------>|
	                                        |--> file..                               |
											                         partition2       |
								....................--------> fetch------------------>|---->merge--->reducer--->output
								                                     partition2       |
								....................--------> fetch------------------>|
											  
	buffer in memory :	本地内存区域。默认100M(io.sort.mb)，map输出的内容很可能大于100M，也就是缓存不下。
						具体来说，达到阈值0.8(io.sort.spil.percent)，就属于达到缓存顶点。
						缓存不下就溢出到本地磁盘中的小文件。小文件大小有限制，写满一个小文件就写一个新的小文件。
						写在这个磁盘文件时，已经分完组，同一组里面也已经排序。一个文件里面会存在多个分组数据的情况。
	fetch : 在map全部处理完后，将所有小文件整合在fetch中，将会对其分组并且重新排序。
	reducer :	将所有map的i组，传递给第i个reducer，由于它们分别有序，全局并非有序。
				因此将在reducer的磁盘上进行排序，排完序后传递给reducer逻辑处理，最后输出。
	注意，REDUCER本身并不知道自己是什么组，不同的组在不同的MAPPER的fetch中在哪里，范围是多少。中间的一切MAPPER和REDUCER的信息交互，都依赖于MRAppMaster.
	可以看到，MAP要反复处理磁盘文件，会相当慢。因此hadoop的mapreduce适合大数据，但是非实时。
	
	2).mapreduce机制概述:
		a.MRAPPMaster启动各个node manager。
		b.node manager进行map task。
		c.执行完成的nm，向MRAPPMaster汇报状态：结果文件所在位置 分区的信息。
		d.MRAPPMaster启动Reduce。
		e.通知对应的reduce要取哪一个分组的数据，以及数据的位置。
		f.读取(下载)所有的nm的对应分组数据，然后将其整合。
		g.reduce task处理数据。
		h.输出到文件。
		
	3).mapreduce其他组件：
		IO:
			InputFormat: 接口。有个默认的InputFormat，处理方式就是一行一行读取，key是起始偏移量，value是这一行的数据。
			OutputFormat:接口。 将有个默认的OutputFormat，这里面包括了写出的文件名。文件的位置默认也是在hdfs中。
			可以换自定义这些IO组件。
		
4.并发map
	在计算mapreduce计算计算文件夹时，这个文件夹下的所有文件都会被计算。
	一个block对应一个map进程效率会很低。因为会有很多小文件，这些小文件每个都启动一个map进程，就太浪费了。
	一个map task应该处理一个相对较大的数据。当一个块较小时，应该处理多个块，当块较大时，就处理一个块。
		1).map task的并发数，是由切片的数量决定。有多少个切片，就启动多少个map task。
		2).切片是逻辑概念，指的就是文件中数据的偏移量范围。
		3).切片的具体大小应该根据所处理的文件的大小来调整。文件小，则应包含多个block，否则就是真实block。
		4).若某nodemanager的小文件总和都很小，那么会去其他datanode取block。
	Mapper中的Context context中包含了切片信息。切片信息包含了文件信息。
		FileSplit split = (FileSplit)context.getInputSplit();				获得当前mapper的切片信息

5.
	int maps=writeSplits(job, submitJobDir);	//切片规划
		->maps=writeNewSplits(job, jobSubmitDir);
			->InputFormat<?,?> input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf)	InuputFormatClas默认是TextInputFormat
			->List<InputSplit> splits = input.getSplits(job); 切片信息，每个切片的偏移量、切片里面的块等等。
				->minSize=Math.max(getFormatMinSplitSize(), getMinSplitSize(job));		切片的最小值，配置参数决定，最小是1。
				->maxSize=getMaxSplitSize();		切片的最大值，由配置参数决定，若没配，则用long的最大值代替。
				->List<InputSplit> splits = new ArrayList<>();
				->List<FileStatus> files = listStatus(job);		找到job要处理的目录中的所有文件。
				->for(FileStatus file : files){					对每一个文件规划切片
					Path path = file.getPath();
					long length=file.getLength;
					blkLocations = file.getBlockLocations();	获得文件块位置信息
					->blSize = file.getBlockSize();				获得块大小
					->splitSize = computeSplitSize(blocksSize, minSize, maxSize);	获得切片大小
						->return Math.max(minSize, Math.Min(maxSize, blocksSize));	相当于是通过minSize和maxSize对splitSize进行了限幅
					->splits.add(makeSplit(...), blkLocations[blkIndex].getHots());	该block的信息，添加到切片中。
				}
				->return splits;
			->jobSplitWriter.createSplitFiles(...);	将切片信息写到了job的提交路径。