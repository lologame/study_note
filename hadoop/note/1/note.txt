hdfs		海量数据的存储
MapReduce	海量数据的分析
YARN		资源管理调度


namenode负责：
接收用户操作请求
维护文件系统的目录结构
管理文件与block之间关系，block与datanode之间关系
datanode负责：
存储文件
文件被分成block存储在磁盘上
为保证数据安全，文件会有多个副本


centos的直接传输:
scp -r source-file-path ip:dest-file-path
-r指的是传输文件夹

初始系统配置：
计算机的虚拟网卡，是和VMware中的虚拟机的网卡，是连接到VMWare中的虚拟网关的。
很多时候，物理机的虚拟网卡ip地址和虚拟网关的ip地址相同，也就是说物理机作了虚拟网关。（也就是物理机是路由器）
不过，有时物理机也只是处于子网中的一个。
虚拟网关、子网、网段均可以在VMWare中配置。
不管如何，物理机和虚拟机同属同一个局域网中。

对于拷贝的虚拟机，会提示该虚拟机是拷贝的还是移动的。
	拷贝的，意味着别处仍然存在该虚拟机，这样Copied It会改变虚拟网卡，以免发生冲突。虚拟网卡是改变ethx来进行的，从eth0-eth7. 要重新配置新的ip地址和mac地址(UUID)。
	移动的，意味着只此一台，这样会和原来得数据完全一模一样，包括MAC。这样就会MAC冲突，MAC相同的主机，外部是ping不通的，因为不知道选择哪一台。


centos关闭图形界面，init 3。

普通用户登录的时候，通过su指令，可以切换到root，只能通过exit回到普通用户。
若是想单条指令用root权限，执行完成后恢复普通用户，就使用sudo。这样可以增强安全。

vi /etc/inittab, 改变启动的级别。3-多用户模式无图形界面 6-图形界面
shift+G	翻到最后
gg 翻到最前面
:q!, 不保存强制退出
对于sudo不支持的的文件，要切换到root，然后vim /etc/sudoers
改完配置文件后，不会自动生效，要重启linux。

由于要用集群，就要设定主机名，于是就涉及到了更改主机名。
通过主机名来寻找主机，而不是通过直接ip地址，这样主机名才有意义。于是要修改本地dns。
sudo vim /etc/hosts

JAVA:
java安装好后，java的指令都在./bin/文件中, 为了方便，要在环境变量中进行设置。
sudo vi /etc/profile
export JAVA_HOME=/home/hadoop/app/jdk1.7.0_65
export PATH=$PATH:$JAVA_HOME/bin
通过source /etc/profile

HADOOP:
bin,  很多执行脚本文件
sbin, 系统的执行脚本文件
etc, 配置文件
share/hadoop/, 中有整个生态圈。
share/hadoop/common/
share/hadoop/hdf/
share/hadoop/mapreduce/
share/hadoop/tools/
share/hadoop/yarn/
share/hadoop/https
配置修改：
hadoop-env.sh, 修改它的JAVA_HMOE的环境变量获取。
core-site.xml，设置hadoop的默认分布式文件系统(同时指定了namenode主机)，以及namenode数据保存位置( 包括datanode以及namenode)。
hdfs-site.xml，设置hadoop的hdfs的数值。replication，副本数（一个文件拷贝的数量，为了增强稳定性）
mapreduce-site.xml, 配置资源管理机制
yarn-site.xml,	设置yarn的资源管理机（老大），map和reduce的传输机制
slaves, 配置启动datanode的机器，默认本机。
启动：
./bin/hadoop namenode -format, 将namenode格式化。
在./sbin/中有各种框架的启动脚本。
start-dfs.sh, 启动后将陆续启动namenode datanode secondaryNameNode.
start-yarn.sh,陆续启动resourcemanager, nodemanager.
以上启动的这些都是计算机，因此启动一个就要输入对应的计算机密码。伪分布式的启动的计算机都是本地计算机。
使用：
可以通过浏览器浏览http://weekend110:500070, 这是resourcemanager主机。
hadoop fs -put file hdfs://weekend110:9000/, 直接上传到根目录。
hadoop fs -get file hdfs://weekend110:9000/, 从根目录下载文件。(可将将hdfs根目录简写为"/")
(hdfs://weekend110是管理集群的主机，它负责将文件访问、分割等操作的分配)

hdfs基本概念:
1.对于写，
指定文件路径名，包括本地和远程，将会将文件进行切割，划分为n块，将每个块放在datanode。
每个datanode计算机都有个datanode进程。datanode将会把数据块放在指定的数据保存位置。
每一个block可以有多个副本，存储在不同的主机中。这样可以提高并发能力，以及数据稳定性。

2.对于读，
指定远程文件路径名，namenode管理文件名对应的块以及这些块所在的主机。
管理主机将会把这些块找到集合起来，返回给客户端。

hdfs shell:
-appendToFile, 将本地文件内容追加到指定hdfs的文件的末尾中。需要注意的是，hdfs中的文件内容不可修改，只可追加。
-cat
-checksum
-chgrp, 改用户组
-chown, 改所属用户，甚至可以改用户组(-chown user:group file)
-chmod
-copyFromLocal, 和-put一样.
-copyToLocal, 和-get一样
-count, 统计指定路径的文件数
-cp, hdfs内部中的文件拷贝
-createSnapshot， 创建文件快照
-deleteSnapshot, 删除文件快照
-df，查看指定指定路径的磁盘使用情况。
-mkdir，创建文件夹
-rm

实际中，通过java api去调用。

ssh登录：
源主机进行密钥对生产：ssh-keygen, 生产密钥对（公钥 私钥）。
生成的公钥，添加到目标主机的authorized_keys中。(cat 公钥 >> 授权列表)
要保证authorized_keys的权限为600(chmod 600 授权列表)
authorized_keys就是授权列表来着。它自动就在跑了，不需要还把它添加在别的地方。