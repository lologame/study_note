hadoop配置：
1.hadoop-env.sh
	修改它的JAVA_HMOE的环境变量获取。
2.core-site.xml
	1).设置hadoop的默认分布式文件系统(同时指定了namenode主机)
	2).namenode数据保存位置( 包括datanode以及namenode)。
	<configuration>
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://weekend110:9000</value>
		</property>
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/home/hadoop/app/hadoop-2.4.1/data/</value>
		</property>
	</configuration>

3.hdfs-site.xml
	设置hadoop的replication(副本)
	<configuration>
	<property>
	<name>dfs.replication</name>
	<value>1</value>
	</property>
	</configuration>

4.mapreduce-site.xml
	配置资源管理机制
	<configuration>
	<property>
	<name>mapreduce.framework.name</name>
	<value>yarn</value>
	</property>
	</configuration>

5.yarn-site.xml
	1).设置yarn的资源管理主机（老大）
	<configuration>
	<property>
	<name>yarn.resourcemanager.hostname</name>
	<value>weekend110</value>
	</property>

	<property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
	</property>

	<property>
	<name>yarn.log-aggregation-enable</name>
	<value>true</value>
	</property>

	<property>
	<name>yarn.log-aggregation.retain-seconds</name>
	<value>604800</value>
	</property>
	</configuration>

6.slaves, 配置启动datanode的机器，默认本机。
7.配置环境变量，将./bin和./sbin添加至环境变量。

hadoop启动:
start-dfs.sh, 启动namenode主机，datanode主机，secondaryNameNode主机。
start-yarn.sh, 启动sourcemanager进程 和 nodemanager进程， 用于mapreduce的。

ssh配置：
ssh-keygen, 生产ssh的密钥对。
touch authorized_keys, 生产授权列表
cat ./id_rsa.pub >> ./authorized_keys, 将公钥添加至授权列表，允许来自生成该公钥的主机访问（生产该公钥的主机就是拥有秘钥的主机）
